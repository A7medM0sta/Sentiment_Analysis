{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahmedmostafa/miniconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27481 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "None\n",
      "sentiment\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahmedmostafa/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/ahmedmostafa/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/ahmedmostafa/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'solver': 'lbfgs'}\n",
      "Accuracy: 0.694742586865563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.61      0.67      1562\n",
      "     neutral       0.63      0.75      0.68      2230\n",
      "    positive       0.77      0.70      0.74      1705\n",
      "\n",
      "    accuracy                           0.69      5497\n",
      "   macro avg       0.71      0.69      0.70      5497\n",
      "weighted avg       0.70      0.69      0.70      5497\n",
      "\n",
      "Support Vector Machine\n",
      "Best Parameters: {'C': 1, 'kernel': 'linear'}\n",
      "Accuracy: 0.7020192832454066\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.61      0.67      1562\n",
      "     neutral       0.63      0.77      0.70      2230\n",
      "    positive       0.80      0.70      0.74      1705\n",
      "\n",
      "    accuracy                           0.70      5497\n",
      "   macro avg       0.72      0.69      0.70      5497\n",
      "weighted avg       0.71      0.70      0.70      5497\n",
      "\n",
      "Random Forest\n",
      "Best Parameters: {'max_depth': None, 'n_estimators': 300}\n",
      "Accuracy: 0.6829179552483172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.54      0.62      1562\n",
      "     neutral       0.61      0.77      0.68      2230\n",
      "    positive       0.77      0.70      0.73      1705\n",
      "\n",
      "    accuracy                           0.68      5497\n",
      "   macro avg       0.71      0.67      0.68      5497\n",
      "weighted avg       0.70      0.68      0.68      5497\n",
      "\n",
      "Simple Neural Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 18:47:23.962446: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-07-19 18:47:23.962573: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-07-19 18:47:23.962584: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-07-19 18:47:23.963143: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-19 18:47:23.963622: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x1484c2980> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x1484c2980>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x1484c2980> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x1484c2980>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"tweet-sentiment-extraction/train.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data[\"sentiment\"].value_counts())\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "data[\"Cleaned_Text\"] = data[\"text\"].apply(lambda x: preprocess_text(x))\n",
    "# data[\"text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Encode the sentiment labels\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "data[\"sentiment\"] = data[\"sentiment\"].map(label_mapping)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[\"Cleaned_Text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature Extraction using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "\n",
    "# Machine Learning Models\n",
    "def train_evaluate_model(model, param_grid, X_train, X_test, y_train, y_test):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring=\"accuracy\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Logistic Regression\")\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_params = {\"C\": [0.1, 1, 10], \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]}\n",
    "train_evaluate_model(\n",
    "    log_reg, log_reg_params, X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    ")\n",
    "\n",
    "# Support Vector Machine\n",
    "print(\"Support Vector Machine\")\n",
    "svm = SVC()\n",
    "svm_params = {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]}\n",
    "train_evaluate_model(svm, svm_params, X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "\n",
    "# Random Forest\n",
    "print(\"Random Forest\")\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20]}\n",
    "train_evaluate_model(rf, rf_params, X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "\n",
    "# Deep Learning Models\n",
    "# Tokenization and Padding for Deep Learning\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "# Simple Neural Network\n",
    "print(\"Simple Neural Network\")\n",
    "model_nn = Sequential(\n",
    "    [\n",
    "        Embedding(input_dim=5000, output_dim=64, input_length=100),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_nn.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model_nn.fit(\n",
    "    X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test)\n",
    ")\n",
    "\n",
    "loss, accuracy = model_nn.evaluate(X_test_pad, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# LSTM Model\n",
    "print(\"LSTM Model\")\n",
    "model_lstm = Sequential(\n",
    "    [\n",
    "        Embedding(input_dim=5000, output_dim=64, input_length=100),\n",
    "        LSTM(64),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_lstm.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model_lstm.fit(\n",
    "    X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test)\n",
    ")\n",
    "\n",
    "loss, accuracy = model_lstm.evaluate(X_test_pad, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
